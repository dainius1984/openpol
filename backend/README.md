# OpenPol Chat Backend API

Backend API server for OpenPol Chat z obsug r贸偶nych provider贸w AI.

##  Szybki Start

### Opcja 1: OpenAI API (Rekomendowane - najprostsze)

1. **Zdobd藕 klucz API z OpenAI:**
   - Zarejestruj si na https://platform.openai.com/
   - Utw贸rz klucz API w sekcji API Keys
   - Masz $5 darmowych kredyt贸w na start

2. **Skonfiguruj zmienne rodowiskowe:**
   ```bash
   cd backend
   cp .env.example .env
   ```
   
   Edytuj `.env` i dodaj sw贸j klucz:
   ```
   OPENAI_API_KEY=sk-your-key-here
   ```

3. **Zainstaluj zale偶noci:**
   ```bash
   pip install -r requirements.txt
   ```

4. **Uruchom serwer:**
   ```bash
   python server.py
   ```

Serwer bdzie dostpny na `http://localhost:5000`

### Opcja 2: Ollama (Lokalny model - darmowy)

1. **Zainstaluj Ollama:**
   - Windows: https://ollama.com/download
   - Mac/Linux: `curl https://ollama.ai/install.sh | sh`

2. **Uruchom model:**
   ```bash
   ollama run llama2
   # lub inny model: ollama run mistral
   ```

3. **Skonfiguruj backend:**
   ```bash
   cd backend
   cp .env.example .env
   ```
   
   Edytuj `.env`:
   ```
   USE_OLLAMA=true
   OLLAMA_URL=http://localhost:11434
   OLLAMA_MODEL=llama2
   ```

4. **Uruchom serwer:**
   ```bash
   python server.py
   ```

### Opcja 3: Mock Responses (Bez API - tylko testy)

Jeli nie skonfigurujesz 偶adnego API, backend automatycznie u偶yje prostych odpowiedzi mockowych.

##  Environment Variables

Utw贸rz plik `.env` w folderze `backend/`:

```env
# OpenAI (Rekomendowane)
OPENAI_API_KEY=sk-your-key-here
OPENAI_MODEL=gpt-3.5-turbo

# Ollama (Opcjonalne)
USE_OLLAMA=false
OLLAMA_URL=http://localhost:11434
OLLAMA_MODEL=llama2

# Server
PORT=5000
DEBUG=false
```

##  API Endpoints

### POST /api/chat

Wysyanie wiadomoci i otrzymywanie odpowiedzi AI.

**Request:**
```json
{
  "messages": [
    {
      "sender": "user",
      "text": "Cze!"
    }
  ]
}
```

**Response:**
```json
{
  "message": "Cze! Jak mog pom贸c?",
  "status": "success"
}
```

### GET /api/health

Sprawdzenie statusu serwera.

**Response:**
```json
{
  "status": "healthy",
  "service": "openpol-chat-api"
}
```

##  Koszty

- **OpenAI GPT-3.5-turbo**: ~$0.002 za 1K token贸w (~$0.01 za 10 wiadomoci)
- **OpenAI GPT-4**: ~$0.03 za 1K token贸w
- **Ollama**: Darmowe (lokalnie)

##  Konfiguracja Frontendu

Upewnij si, 偶e frontend wie gdzie jest backend. W pliku `.env` w g贸wnym folderze projektu:

```env
REACT_APP_CHAT_API_URL=http://localhost:5000
```

##  Troubleshooting

### Bd: "OPENAI_API_KEY nie jest ustawiony"
- Sprawd藕 czy plik `.env` istnieje w folderze `backend/`
- Sprawd藕 czy klucz API jest poprawny
- Uruchom ponownie serwer po dodaniu zmiennych

### Bd: "Nie mo偶na poczy si z serwerem"
- Sprawd藕 czy serwer dziaa: `python server.py`
- Sprawd藕 czy port 5000 jest wolny
- Sprawd藕 CORS settings

### Ollama nie dziaa
- Sprawd藕 czy Ollama jest uruchomiona: `ollama list`
- Sprawd藕 czy model jest pobrany: `ollama pull llama2`
- Sprawd藕 URL w `.env`

##  Dostpne Modele

### OpenAI:
- `gpt-3.5-turbo` (najtaszy, szybki)
- `gpt-4` (najlepszy, dro偶szy)
- `gpt-4-turbo` (kompromis)

### Ollama:
- `llama2` (dobry, uniwersalny)
- `mistral` (szybki, dobry)
- `codellama` (dla kodu)
- `llama2:13b` (wikszy, lepszy)

##  Production Deployment

1. U偶yj zmiennych rodowiskowych na serwerze
2. Ustaw `DEBUG=false`
3. U偶yj reverse proxy (nginx)
4. Dodaj rate limiting
5. Monitoruj koszty API

##  Nastpne Kroki

1. Wybierz provider (OpenAI/Ollama)
2. Skonfiguruj `.env`
3. Uruchom backend
4. Przetestuj chat w przegldarce
5. Monitoruj koszty i jako odpowiedzi
